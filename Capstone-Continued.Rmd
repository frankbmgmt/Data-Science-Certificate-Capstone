---
title: "Capstone-Continued"
author: "Scott Stoltzman"
date: "7/30/2019"
output:
  html_document:
    code_folding: "show"
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library("stringi")
library("tidyverse")
library("tidytext")
library("scales")
library('caret')
library('SnowballC')
library('syuzhet')
data("stop_words")
set.seed(123)
```


# Load Data
```{r}
# raw_dat = read_csv('IMDB Dataset.csv') # can use this if you need to have the file stored locally, just place it into the top level directory of this project
# change to reviewer recommended yes/no for ease of differentiating sentiment later on
raw_dat = read_csv('https://foco-ds-portal-files.s3.amazonaws.com/IMDB+Dataset.csv') %>%
  rename(reviewer_recommended = sentiment) %>%
  mutate(reviewer_recommended = str_replace(reviewer_recommended, 'positive', 'yes'),
         reviewer_recommended = str_replace(reviewer_recommended, 'negative', 'no')) %>%
  mutate(id = row_number())
```

#
```{r}
# Swear words:
swear_words_df = read_csv('SwearWords.csv')
swear_words = c()
for(i in swear_words_df$arse){
  swear_words = c(swear_words, i)
  }
swear_words = paste(swear_words, collapse('|'))
```


```{r}
dat_sentiment = raw_dat %>% 
  mutate(number_of_words = stri_count(review, regex="\\S+"),
         number_of_letters = length(review),
         avg_word_length = number_of_letters / number_of_words,
         sentiment = get_sentiment(review),
         wrd_hate = as.numeric(str_detect(review, 'hate')),
         wrd_love = as.numeric(str_detect(review, 'love')),
         wrd_waste = as.numeric(str_detect(review, 'waste')),
         wrd_worth = as.numeric(str_detect(review, 'worth')),
         swr_wrds = as.numeric(str_detect(review, swear_words))) %>%
  select(-id, -review)

dat_sentiment %>% 
  ggplot(aes(x = sentiment)) +
  geom_density(aes(col=reviewer_recommended, fill=reviewer_recommended), alpha=0.5) + 
  ggtitle('Review Sentiment Density')
```

```{r}
training_split = 0.75
smp_size = floor(training_split * nrow(dat_sentiment))
dat_index = sample(seq_len(nrow(dat_sentiment)), size = smp_size)
dat_train = as.data.frame(dat_sentiment[dat_index,])
dat_test = as.data.frame(dat_sentiment[-dat_index,])
```

```{r}
train_control = trainControl(method = "oob")

mod_rf = train(dat_train %>% select(-reviewer_recommended),
            dat_train$reviewer_recommended,
            method = "ranger",
            num.trees = 50,
            importance = "impurity",
            trControl = train_control)

predictions_rf = predict(mod_rf, dat_test)
confusionMatrix(predictions_rf, as.factor(dat_test$reviewer_recommended))
```

```{r}
mod_rf$finalModel %>%
  # extract variable importance metrics
  ranger::importance() %>%
  # convert to a data frame
  enframe(name = "variable", value = "varimp") %>%
  top_n(n = 20, wt = varimp) %>%
  # plot the metrics
  ggplot(aes(x = fct_reorder(variable, varimp), y = varimp)) +
  geom_col() +
  coord_flip() +
  labs(x = "Token",
       y = "Variable importance (higher is more important)")
```


## Neural Network
```{r, message=FALSE}
train_control = trainControl(method = 'cv',
                     savePredictions = TRUE,
                     classProbs = TRUE)
mod_nnet = train(dat_dtm_reduced_matrix, 
                dat_train$reviewer_recommended, 
                method = "nnet",
                trace = FALSE,
                trControl = train_control)
predictions_nnet = predict(mod_nnet$finalModel, dat_dtm_reduced_matrix_predict, 
                          type = 'class')
confusionMatrix(as.factor(predictions_nnet), as.factor(can_predict$reviewer_recommended))

