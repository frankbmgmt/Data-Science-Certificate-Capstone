---
title: "Capstone-Continued"
author: "Scott Stoltzman"
date: "7/30/2019"
output: html_document
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("tidyverse")
library("tidytext")
library("scales")
library('caret')
library('SnowballC')
data("stop_words")
set.seed(123)
```

```{r}
# raw_dat = read_csv('IMDB Dataset.csv') # can use this if you need to have the file stored locally, just place it into the top level directory of this project
# change to reviewer recommended yes/no for ease of differentiating sentiment later on
raw_dat = read_csv('https://foco-ds-portal-files.s3.amazonaws.com/IMDB+Dataset.csv') %>%
  rename(reviewer_recommended = sentiment) %>%
  mutate(reviewer_recommended = str_replace(reviewer_recommended, 'positive', 'yes'),
         reviewer_recommended = str_replace(reviewer_recommended, 'negative', 'no')) %>%
  mutate(id = row_number())
```


```{r}
afinn = get_sentiments("afinn")
bing = get_sentiments("bing")
loughran = get_sentiments("loughran")
```

```{r}
dat_bigrams = raw_dat %>%
  unnest_tokens(bigram, review, token = "ngrams", n = 2)

bigrams_separated = dat_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

custom_stop_words = c("br")

bigrams_filtered = bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  filter(!word1 %in% custom_stop_words) %>%
  filter(!word2 %in% custom_stop_words)

bigram_counts = bigrams_filtered %>%
  count(word1, word2, sort = TRUE)
```


```{r}
negation_words <- c("not", "no", "never", "without")

negated_words <- bigrams_separated %>%
  filter(word1 %in% negation_words) %>%
  inner_join(afinn, by = c(word2 = "word")) %>%
  count(word1, word2, value, sort = TRUE) %>%
  left_join(bigrams_separated, by = c('word1' = 'word1', 'word2' = 'word2')) %>%
  group_by(id) %>%
  summarize(sentiment = mean(value))

non_negated_words = bigrams_separated %>%
  anti_join(negated_words, by = 'id') %>%
  inner_join(afinn, by = c(word2 = "word")) %>%
  count(word1, word2, value, sort = TRUE) %>%
  left_join(bigrams_separated, by = c('word1' = 'word1', 'word2' = 'word2')) %>%
  group_by(id) %>%
  summarize(sentiment = mean(value))

modified_sentiment_prep = bind_rows(negated_words, non_negated_words) %>%
  group_by(id) %>%
  summarize(sentiment = mean(sentiment))

modified_sentiment = tibble(id = 1:max(raw_dat$id)) %>%
  left_join(modified_sentiment_prep, by = 'id')

modified_sentiment[is.na(modified_sentiment)] = 0 #assumption
```




```{r}
training_split = 0.70
smp_size = floor(training_split * nrow(raw_dat))
dat_index = sample(seq_len(nrow(raw_dat)), size = smp_size)
dat_train = as.data.frame(raw_dat[dat_index,])
dat_test = as.data.frame(raw_dat[-dat_index,])

tmp = dat_train

dat = tmp %>%
  unnest_tokens(word, review) %>%
  anti_join(stop_words, by = 'word') %>%
  mutate(word = str_extract(word, "[a-z']+")) %>%
  filter(word != 'br') %>%
  filter(word != 'movie') %>%
  filter(word != 'film')

tmp %>% anti_join(modified_sentiment, by = 'id')

dat_sentiment = tmp %>%
  left_join(modified_sentiment, by = 'id')
  
dat_dtm = dat %>%
  count(id, word) %>%
  cast_dtm(document = id, term = word, 
           value = n, weighting = tm::weightTfIdf)

dat_dtm_reduced = tm::removeSparseTerms(dat_dtm, sparse = 0.95)

dat_dtm_reduced_matrix = cbind(as.matrix(dat_dtm_reduced), 
                               data_sentiment = dat_sentiment$sentiment)

mod = train(x = dat_dtm_reduced_matrix,
            y = dat_train$reviewer_recommended,
            method = "ranger",
            num.trees = 50,
            importance = "impurity",
            trControl = trainControl(method = "oob"))
mod
```

```{r}
mod$finalModel
```


```{r}
mod$finalModel %>%
  # extract variable importance metrics
  ranger::importance() %>%
  # convert to a data frame
  enframe(name = "variable", value = "varimp") %>%
  top_n(n = 20, wt = varimp) %>%
  # plot the metrics
  ggplot(aes(x = fct_reorder(variable, varimp), y = varimp)) +
  geom_col() +
  coord_flip() +
  labs(x = "Token",
       y = "Variable importance (higher is more important)")
```


```{r}
dat_predict = dat_test %>%
  unnest_tokens(word, review) %>%
  filter(word %in% dat_dtm_reduced$dimnames$Terms)

can_predict = dat_test %>%
  filter(id %in% unique(dat_predict$id)) 

dat_sentiment_predict = can_predict %>%
  left_join(modified_sentiment, by = 'id')

dat_dtm_predict = dat_predict %>%
  count(id, word) %>%
  cast_dtm(document = id, term = word, 
           value = n, weighting = tm::weightTfIdf)

dat_dtm_reduced_predict = tm::removeSparseTerms(dat_dtm_predict, sparse = 0.99)

dat_dtm_reduced_matrix_predict = cbind(as.matrix(dat_dtm_reduced_predict),
                                       data_sentiment = dat_sentiment_predict$sentiment)

predictions = predict(mod, dat_dtm_reduced_matrix_predict)
confusionMatrix(predictions, as.factor(can_predict$reviewer_recommended))
```