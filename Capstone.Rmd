---
title: "Classifying IMDB Reviews"
author: "Scott Stoltzman"
date: "7/22/2019"
output: html_document
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("tidyverse")
library("tidytext")
library("scales")
library("wordcloud")
library("reshape2")
data("stop_words")
```

# Add cool title up here
#### Action plan, business case, high level summary of results, etc. (all go somewhere up top)

Data is coming from: <https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews/kernels>

## Exploratory Data Analysis
```{r}
# raw_dat = read_csv('IMDB Dataset.csv') # can use this if you need to have the file stored locally, just place it into the top level directory of this project
# change to reviewer recommended yes/no for ease of differentiating sentiment later on
raw_dat = read_csv('https://foco-ds-portal-files.s3.amazonaws.com/IMDB+Dataset.csv') %>%
  rename(reviewer_recommended = sentiment) %>%
  mutate(reviewer_recommended = str_replace(reviewer_recommended, 'positive', 'yes'),
         reviewer_recommended = str_replace(reviewer_recommended, 'negative', 'no'))
  
head(raw_dat)
```

```{r}
raw_dat %>%
  unnest_tokens(word, reviewer_recommended) %>%
  head()
```


```{r}
dat = raw_dat %>%
  unnest_tokens(word, review) %>%
  anti_join(stop_words, by = 'word')
```


```{r}
dat %>%
  count(word, sort = TRUE)
```


Notice that 'br' was for making a new line `<br>`
```{r}
dat = raw_dat %>%
  unnest_tokens(word, review) %>%
  anti_join(stop_words, by = 'word') %>%
  mutate(word = str_extract(word, "[a-z']+")) %>%
  filter(word != 'br') %>%
  filter(word != 'movie') %>%
  filter(word != 'film')
head(dat)
```


```{r}
dat %>%
  count(word, sort = TRUE) %>%
  summary()
```

```{r}
dat %>%
  count(word, sort = TRUE) %>%
  filter(n > 10000) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()
```


```{r}
frequency = dat %>% 
  count(reviewer_recommended, word) %>%
  group_by(reviewer_recommended) %>%
  mutate(proportion = n / sum(n)) %>%
  select(-n) %>%
  spread(reviewer_recommended, proportion) 
head(frequency)
```


```{r}
ggplot(frequency, aes(x = no, y = yes, color = abs(yes - no))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
  theme(legend.position="none") +
  labs(y = "Word", x = NULL)
```


```{r}
cor.test(x = frequency$no, y = frequency$yes)
```



Sentiment has already been done "per word" in a number of data sets
```{r}
afinn = get_sentiments("afinn")
bing = get_sentiments("bing")
loughran = get_sentiments("loughran")
```


```{r}
sent_dat = dat %>%
  left_join(bing, by = 'word') %>%
  rename(bing = sentiment) %>%
  left_join(loughran, by = 'word') %>%
  rename(loughran = sentiment)
head(sent_dat)
```


# What's our yes/no split?
```{r}
sent_dat %>%
  group_by(reviewer_recommended) %>%
  summarize(n = n()) %>%
  ggplot(aes(x = reviewer_recommended, y = n)) + 
  geom_col()
```


```{r}
sent_dat %>%
  count(word, reviewer_recommended, sort = TRUE) %>%
  acast(word ~ reviewer_recommended, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = brewer.pal(8,"Dark2"),
                   max.words = 100)
```

```{r}
sent_dat %>%
  filter(!is.na(bing)) %>%
  count(word, bing, sort = TRUE) %>%
  acast(word ~ bing, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = brewer.pal(8,"Dark2"),
                   max.words = 100)
```







```{r}
sent_dat %>%
  filter(!is.na(loughran)) %>%
  count(word, loughran, sort = TRUE) %>%
  acast(word ~ loughran, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = brewer.pal(8,"Dark2"),
                   max.words = 100)
```




## Looking beyond individual words!

```{r}
dat_sentences = raw_dat %>% 
  rename(text = review) %>%
  unnest_tokens(sentence, text, token = "sentences")
head(dat_sentences)
```


```{r}
sentence_dat = raw_dat %>%
  mutate(id = row_number()) %>%
  unnest_tokens(word, review) %>%
  # anti_join(stop_words, by = 'word') %>%
  mutate(word = str_extract(word, "[a-z']+")) %>%
  filter(word != 'br') %>%
  filter(word != 'movie') %>%
  filter(word != 'film')

sentence_word_counts <- sentence_dat %>%
  group_by(reviewer_recommended, id) %>%
  summarize(words = n())

head(sentence_word_counts)
```

```{r}
# Most negative
sentence_dat %>%
  semi_join(bing %>% filter(sentiment == 'negative'), by = 'word') %>%
  group_by(reviewer_recommended, id) %>%
  summarize(negativewords = n()) %>%
  left_join(sentence_word_counts, by = c("reviewer_recommended", "id")) %>%
  mutate(ratio = negativewords/words) %>%
  top_n(1) %>%
  ungroup()
```


```{r}
# Most positive
sentence_dat %>%
  semi_join(bing %>% filter(sentiment == 'positive'), by = 'word') %>%
  group_by(reviewer_recommended, id) %>%
  summarize(positivewords = n()) %>%
  left_join(sentence_word_counts, by = c("reviewer_recommended", "id")) %>%
  mutate(ratio = positivewords/words) %>%
  top_n(1) %>%
  ungroup()
```


## Identifying important words

```{r}
dat_word_count = dat %>%
  count(reviewer_recommended, word, sort = TRUE)
head(dat_word_count)
```


```{r}
dat_total_count = dat_word_count %>%
  group_by(reviewer_recommended) %>%
  summarize(total = sum(n))
head(dat_total_count)
```

```{r}
dat_word_count = dat_word_count %>%
  left_join(dat_total_count, by = 'reviewer_recommended')
head(dat_word_count)
```


```{r}
ggplot(dat_word_count, aes(n/total, fill = reviewer_recommended)) +
  geom_histogram(show.legend = FALSE) +
  xlim(NA, 0.0009) +
  facet_wrap(~reviewer_recommended, ncol = 2, scales = "free_y")
```

```{r}
freq_by_rank <- dat_word_count %>% 
  group_by(reviewer_recommended) %>% 
  mutate(rank = row_number(), 
         `term frequency` = n/total)

head(freq_by_rank)
```

```{r}
freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = reviewer_recommended)) + 
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10()
```


```{r}
rank_subset <- freq_by_rank %>% 
  filter(rank < 1000,
         rank > 5)

lm(log10(`term frequency`) ~ log10(rank), data = rank_subset)
```


```{r}
freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = reviewer_recommended)) + 
  geom_abline(intercept = -0.62, slope = -1.1, color = "gray50", linetype = 2) +
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10()
```


```{r}
dat_word_count <- dat_word_count %>%
  bind_tf_idf(word, reviewer_recommended, n)
head(dat_word_count)
```

```{r}
dat_word_count %>%
  select(-total) %>%
  arrange(desc(tf_idf))
```

```{r}
dat_word_count %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  group_by(reviewer_recommended) %>% 
  top_n(15) %>% 
  ungroup() %>%
  ggplot(aes(word, tf_idf, fill = reviewer_recommended)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~reviewer_recommended, ncol = 2, scales = "free") +
  coord_flip()
```


```{r}
dat_word_count = dat_word_count %>%
  group_by(reviewer_recommended) %>% 
  arrange(-tf_idf) %>%
  mutate(ranking = row_number()) %>%
  ungroup()
head(dat_word_count)
```

```{r}
dat_word_count %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  group_by(reviewer_recommended) %>% 
  top_n(15, wt = tf_idf) %>% 
  ungroup() %>%
  ggplot(aes(word, tf_idf, fill = reviewer_recommended)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~reviewer_recommended, ncol = 2, scales = "free") +
  coord_flip()
```



# How do I unnest the raw text data?
```{r}
dat_bigrams = raw_dat %>%
```


```{r}
dat_bigrams = raw_dat %>%
  unnest_tokens(bigram, review, token = "ngrams", n = 2)
head(dat_bigrams)
```

# How do I see the most used bigrams?
```{r}
dat_bigrams %>%
  
```

```{r}
dat_bigrams %>%
  count(bigram, sort = TRUE) %>%
  head()
```



```{r}
bigrams_separated = dat_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

custom_stop_words = c("br")

bigrams_filtered = bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  filter(!word1 %in% custom_stop_words) %>%
  filter(!word2 %in% custom_stop_words)

bigram_counts = bigrams_filtered %>%
  count(word1, word2, sort = TRUE)

head(bigram_counts)
```


```{r}
bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

bigrams_united
```


# Trigram example...
```{r}
raw_dat %>%
  unnest_tokens(trigram, review, token = "ngrams", n = 3) %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
  filter(!word1 %in% c(stop_words$word, custom_stop_words),
         !word2 %in% c(stop_words$word, custom_stop_words),
         !word3 %in% c(stop_words$word, custom_stop_words)) %>%
  count(word1, word2, word3, sort = TRUE)
```

# Analyzing bigrams
```{r}
# What's the most common word before...
bigrams_filtered %>%
  filter(word2 == "movie") %>%
  count(reviewer_recommended, word1, sort = TRUE)
```


```{r}
bigram_tf_idf <- bigrams_united %>%
  count(reviewer_recommended, bigram) %>%
  bind_tf_idf(bigram, reviewer_recommended, n) %>%
  arrange(desc(tf_idf))

bigram_tf_idf
```

```{r}
bigram_tf_idf %>%
  arrange(desc(tf_idf)) %>%
  mutate(bigram = factor(bigram, levels = rev(unique(bigram)))) %>% 
  group_by(reviewer_recommended) %>% 
  top_n(15, wt = tf_idf) %>% 
  ungroup() %>%
  ggplot(aes(bigram, tf_idf, fill = reviewer_recommended)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~reviewer_recommended, ncol = 2, scales = "free") +
  coord_flip()
```


```{r}
bigrams_separated %>%
  filter(word1 == "not") %>%
  count(word1, word2, sort = TRUE)
```


```{r}
not_words <- bigrams_separated %>%
  filter(word1 == "not") %>%
  inner_join(afinn, by = c(word2 = "word")) %>%
  count(word2, value, sort = TRUE)

not_words
```


```{r}
not_words %>%
  mutate(contribution = n * value) %>%
  arrange(desc(abs(contribution))) %>%
  head(20) %>%
  mutate(word2 = reorder(word2, contribution)) %>%
  ggplot(aes(word2, n * value, fill = n * value > 0)) +
  geom_col(show.legend = FALSE) +
  xlab("Words preceded by \"not\"") +
  ylab("Sentiment score * number of occurrences") +
  coord_flip()
```


```{r}
negation_words <- c("not", "no", "never", "without")

negated_words <- bigrams_separated %>%
  filter(word1 %in% negation_words) %>%
  inner_join(afinn, by = c(word2 = "word")) %>%
  count(word1, word2, value, sort = TRUE)

head(negated_words)
```


```{r}
negated_words %>%
  mutate(contribution = n * value) %>%
  arrange(desc(abs(contribution))) %>%
  group_by(word1) %>%
  mutate(ranking = row_number()) %>%
  top_n(10, wt = -ranking) %>%
  arrange(ranking) %>%
  ungroup() %>%
  mutate(word2 = reorder(word2, contribution)) %>%
  ggplot(aes(word2, n * value, fill = n * value > 0)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~word1, ncol = 2, scales = "free") +
  xlab("Words preceded by \"not\"") +
  ylab("Sentiment score * number of occurrences") +
  coord_flip()
```


```{r}

```

