---
title: "Classifying IMDB Reviews"
author: "Scott Stoltzman"
date: "7/22/2019"
output: html_document
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("tidyverse")
library("tidytext")
library("scales")
library("wordcloud")
library("reshape2")
data("stop_words")
```

# Add cool title up here
#### Action plan, business case, high level summary of results, etc. (all go somewhere up top)

Data is coming from: <https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews/kernels>

## Exploratory Data Analysis
```{r}
# raw_dat = read_csv('IMDB Dataset.csv') # can use this if you need to have the file stored locally, just place it into the top level directory of this project
raw_dat = read_csv('https://foco-ds-portal-files.s3.amazonaws.com/IMDB+Dataset.csv')
head(raw_dat)
```

```{r}
raw_dat %>%
  unnest_tokens(word, review) %>%
  head()
```


```{r}
dat = raw_dat %>%
  unnest_tokens(word, review) %>%
  anti_join(stop_words, by = 'word')
```


```{r}
dat %>%
  count(word, sort = TRUE)
```


Notice that 'br' was for making a new line `<br>`
```{r}
dat = raw_dat %>%
  unnest_tokens(word, review) %>%
  anti_join(stop_words, by = 'word') %>%
  mutate(word = str_extract(word, "[a-z']+")) %>%
  filter(word != 'br') %>%
  filter(word != 'movie') %>%
  filter(word != 'film')
```


```{r}
dat %>%
  count(word, sort = TRUE) %>%
  summary()
```

```{r}
dat %>%
  count(word, sort = TRUE) %>%
  filter(n > 10000) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()
```


```{r}
frequency = dat %>% 
  count(sentiment, word) %>%
  group_by(sentiment) %>%
  mutate(proportion = n / sum(n)) %>%
  select(-n) %>%
  spread(sentiment, proportion) 
head(frequency)
```


```{r}
ggplot(frequency, aes(x = negative, y = positive, color = abs(positive - negative))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
  theme(legend.position="none") +
  labs(y = "Word", x = NULL)
```


```{r}
cor.test(x = frequency$negative, y = frequency$positive)
```



Sentiment has already been done "per word" in a number of data sets
```{r}
afinn = get_sentiments("afinn")
bing = get_sentiments("bing")
loughran = get_sentiments("loughran")
```


```{r}
sent_dat = dat %>%
  rename(original_sentiment = sentiment) %>%
  left_join(bing, by = 'word') %>%
  rename(bing = sentiment) %>%
  left_join(loughran, by = 'word') %>%
  rename(loughran = sentiment)
head(sent_dat)
```

compare bing...
```{r}
sent_dat %>%
  filter(!is.na(bing)) %>%
  mutate(difference = if_else(original_sentiment == bing, 1, 0)) %>%
  group_by(original_sentiment) %>%
  add_tally() %>%
  summarize(pct_matching = sum(difference) / max(n))
```


matching loughran
```{r}
sent_dat %>%
  filter(!is.na(loughran)) %>%
  mutate(difference = if_else(original_sentiment == loughran, 1, 0)) %>%
  group_by(original_sentiment) %>%
  add_tally() %>%
  summarize(pct_matching = sum(difference) / max(n))
```

```{r}
sent_dat %>%
  group_by(original_sentiment) %>%
  summarize(n = n()) %>%
  ggplot(aes(x = original_sentiment, y = n)) + 
  geom_col()
```


```{r}
sent_dat %>%
  count(word, original_sentiment, sort = TRUE) %>%
  acast(word ~ original_sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = brewer.pal(8,"Dark2"),
                   max.words = 100)
```

```{r}
sent_dat %>%
  filter(!is.na(bing)) %>%
  count(word, bing, sort = TRUE) %>%
  acast(word ~ bing, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = brewer.pal(8,"Dark2"),
                   max.words = 100)
```







```{r}
sent_dat %>%
  filter(!is.na(loughran)) %>%
  count(word, loughran, sort = TRUE) %>%
  acast(word ~ loughran, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = brewer.pal(8,"Dark2"),
                   max.words = 100)
```




## Looking beyond individual words!

```{r}
dat_sentences = raw_dat %>% 
  rename(text = review) %>%
  unnest_tokens(sentence, text, token = "sentences")
head(dat_sentences)
```


```{r}
sentence_dat = raw_dat %>%
  mutate(id = row_number()) %>%
  unnest_tokens(word, review) %>%
  anti_join(stop_words, by = 'word') %>%
  mutate(word = str_extract(word, "[a-z']+")) %>%
  filter(word != 'br') %>%
  filter(word != 'movie') %>%
  filter(word != 'film') %>%
  rename(original_sentiment = sentiment)

sentence_word_counts <- sentence_dat %>%
  group_by(original_sentiment, id) %>%
  summarize(words = n())

head(sentence_word_counts)
```

```{r}
# Most negative
sentence_dat %>%
  semi_join(bing %>% filter(sentiment == 'negative'), by = 'word') %>%
  group_by(original_sentiment, id) %>%
  summarize(negativewords = n()) %>%
  left_join(sentence_word_counts, by = c("original_sentiment", "id")) %>%
  mutate(ratio = negativewords/words) %>%
  top_n(1) %>%
  ungroup()
```


```{r}
# Most positive
sentence_dat %>%
  semi_join(bing %>% filter(sentiment == 'positive'), by = 'word') %>%
  group_by(original_sentiment, id) %>%
  summarize(positivewords = n()) %>%
  left_join(sentence_word_counts, by = c("original_sentiment", "id")) %>%
  mutate(ratio = positivewords/words) %>%
  top_n(1) %>%
  ungroup()
```




## Identifying important words

```{r}
dat_word_count = dat %>%
  count(sentiment, word, sort = TRUE)
head(dat_word_count)
```


```{r}
dat_total_count = dat_word_count %>%
  group_by(sentiment) %>%
  summarize(total = sum(n))
head(dat_total_count)
```

```{r}
dat_word_count = dat_word_count %>%
  left_join(dat_total_count, by = 'sentiment')
head(dat_word_count)
```


```{r}
ggplot(dat_word_count, aes(n/total, fill = sentiment)) +
  geom_histogram(show.legend = FALSE) +
  xlim(NA, 0.0009) +
  facet_wrap(~sentiment, ncol = 2, scales = "free_y")
```

```{r}
freq_by_rank <- dat_word_count %>% 
  group_by(sentiment) %>% 
  mutate(rank = row_number(), 
         `term frequency` = n/total)

head(freq_by_rank)
```

```{r}
freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = sentiment)) + 
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10()
```


```{r}
rank_subset <- freq_by_rank %>% 
  filter(rank < 1000,
         rank > 5)

lm(log10(`term frequency`) ~ log10(rank), data = rank_subset)
```


```{r}
freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = sentiment)) + 
  geom_abline(intercept = -0.62, slope = -1.1, color = "gray50", linetype = 2) +
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10()
```


```{r}
dat_word_count <- dat_word_count %>%
  bind_tf_idf(word, sentiment, n)
head(dat_word_count)
```

```{r}
dat_word_count %>%
  select(-total) %>%
  arrange(desc(tf_idf))
```

```{r}
dat_word_count %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  group_by(sentiment) %>% 
  top_n(15) %>% 
  ungroup() %>%
  ggplot(aes(word, tf_idf, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~sentiment, ncol = 2, scales = "free") +
  coord_flip()
```


```{r}
dat_word_count = dat_word_count %>%
  group_by(sentiment) %>% 
  arrange(-tf_idf) %>%
  mutate(ranking = row_number()) %>%
  ungroup()
head(dat_word_count)
```

```{r}
dat_word_count %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  group_by(sentiment) %>% 
  top_n(15, wt = tf_idf) %>% 
  ungroup() %>%
  ggplot(aes(word, tf_idf, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~sentiment, ncol = 2, scales = "free") +
  coord_flip()
```

